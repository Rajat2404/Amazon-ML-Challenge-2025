# Amazon-ML-Challenge-2025

****Smart Product Pricing Challenge****


**Methodology Used**
The core of our approach involved developing a text-based price regression model using Transfer Learning within the domain of Natural Language Processing (NLP). Recognizing that the catalog_content—containing the product title, description, and item pack quantity—was the most information-rich feature, we selected a powerful pre-trained Transformer model to capture the complex relationships between text features and the target price. The training process was executed in two distinct, sequential stages. The first stage consisted of an initial fine-tuning run for 8 epochs on the entire dataset to quickly adapt the pre-trained model's general language understanding to the specific e-commerce domain. The second stage involved continuing the fine-tuning for an additional 4 epochs on a cleaned subset of the data. This focused, two-stage fine-tuning approach aimed to maximize the benefits of the pre-trained weights while selectively mitigating the noise introduced by severe price outliers.

**Model Architecture/Algorithms Selected**
The selected model architecture is DeBERTa-v3-base (microsoft/deberta-v3-base), a sophisticated Transformer encoder known for its improved attention mechanism and efficiency. This model, which conforms to the 8 billion parameter constraint, was repurposed from a classification task to a regression task by configuring the output head to predict a single continuous value (num_labels=1). The model was optimized using the AdamW optimizer with a fixed learning rate of  and a weight decay of . The training objective was defined by the Mean Squared Error (MSE) loss function, which is standard for continuous value prediction. An initial exploration of traditional machine learning methods, such as LightGBM, confirmed that the deep learning, transfer-learning based NLP model provided superior feature extraction and predictive performance given the complexity of the textual data.

**Feature Engineering Techniques Applied**
Feature engineering concentrated primarily on preparing the target variable and cleaning the input data for enhanced model stability. Most critically, the raw price target variable was log-transformed using the  function (np.log1p). This transformation is essential for stabilizing variance, converting the highly right-skewed price distribution into a more manageable, quasi-Gaussian distribution, and ensuring that all predictions post-inverse-transformation () are positive, as required by the constraints. Furthermore, a rigorous Outlier Removal step was applied to the training set using the Interquartile Range (IQR) method (filtering data points outside the  and  bounds), which successfully removed approximately 7.4% of extreme price outliers. For the input text, the catalog_content field underwent standard tokenization using the DeBERTa-v3 tokenizer, with sequences padded and truncated to a maximum length of 128 tokens. The image_link feature was not integrated into this final model iteration.
